import streamlit as st
import os
import json
from textblob import TextBlob
from dotenv import load_dotenv
from openai import OpenAI

# ----------------------------------------
# TruthLens â€“ ìµœì‹  OpenAI SDK ê¸°ë°˜ SNS í—ˆìœ„ì •ë³´ íƒì§€ê¸°
# ----------------------------------------

# ğŸ” .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ğŸŒ í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="TruthLens AI", page_icon="ğŸ§ ", layout="centered")

st.write("âœ… TruthLens ì•±ì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")
st.write("ğŸ”‘ API Key ë¡œë“œ ìƒíƒœ:", bool(os.getenv("OPENAI_API_KEY")))

st.title("ğŸ§  TruthLens â€“ GPT ê¸°ë°˜ SNS í—ˆìœ„ì •ë³´ íƒì§€ê¸°")
st.caption("AIê°€ SNS ê²Œì‹œë¬¼ì˜ ì‹ ë¢°ë„, ê³¼ì¥ ì—¬ë¶€, ê°ì • ìƒíƒœë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

# âœï¸ ì‚¬ìš©ì ì…ë ¥
user_post = st.text_area("ê²Œì‹œë¬¼ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”:")

# ğŸ“ ë°ì´í„° í´ë” ìë™ ìƒì„±
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)
LEARNING_FILE = os.path.join(DATA_DIR, "learning_data.json")

# ğŸ§© ê¸°ë³¸ ê°ì • ë¶„ì„ + í‚¤ì›Œë“œ íƒì§€
def detect_post(post):
    suspicious_keywords = ['ì¡°ì‘', 'ì¶©ê²©', 'í­ë¡œ', 'ë¯¿ê¸° í˜ë“ ', 'ì§„ì§œì¼ê¹Œ', 'ìŒëª¨']
    found = [word for word in suspicious_keywords if word in post]
    sentiment = TextBlob(post).sentiment.polarity
    return {"suspicious": bool(found), "keywords": found, "sentiment": sentiment}

# ğŸ¤– GPT-4 Turbo ê¸°ë°˜ ì‹ ë¢°ë„ ë¶„ì„ (ìµœì‹  SDK ë°©ì‹)
def interpret_post_with_gpt(post):
    try:
        response = client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” SNS ê²Œì‹œë¬¼ì˜ ì‹ ë¢°ë„ë¥¼ í‰ê°€í•˜ëŠ” AIì•¼. í—ˆìœ„Â·ê³¼ì¥Â·ì¤‘ë¦½ ì¤‘ í•˜ë‚˜ë¡œ ìš”ì•½í•´ì„œ íŒë‹¨í•´ì¤˜."},
                {"role": "user", "content": post}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        st.error(f"âš ï¸ GPT ë¶„ì„ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "GPT ì˜¤ë¥˜"

# ğŸ’¾ í•™ìŠµ ë°ì´í„° ì €ì¥
def save_learning_data(post, gpt_label, detection_result):
    record = {
        "post": post,
        "GPT_label": gpt_label,
        "suspicious": detection_result["suspicious"],
        "keywords": detection_result["keywords"],
        "sentiment": detection_result["sentiment"]
    }

    try:
        with open(LEARNING_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        data = []

    data.append(record)

    with open(LEARNING_FILE, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

# ğŸš€ ì‹¤í–‰ ë²„íŠ¼
if st.button("ë¶„ì„í•˜ê¸°"):
    if not user_post.strip():
        st.warning("ê²Œì‹œë¬¼ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    else:
        detection_result = detect_post(user_post)
        gpt_label = interpret_post_with_gpt(user_post)
        save_learning_data(user_post, gpt_label, detection_result)

        st.subheader("ğŸ” ë¶„ì„ ê²°ê³¼")
        st.write(f"**íƒì§€ëœ í‚¤ì›Œë“œ:** {', '.join(detection_result['keywords']) if detection_result['keywords'] else 'ì—†ìŒ'}")
        st.write(f"**ê°ì • ì ìˆ˜:** {round(detection_result['sentiment'], 2)}")
        st.write(f"**GPT íŒë‹¨ ê²°ê³¼:** {gpt_label}")

        if any(word in gpt_label for word in ["í—ˆìœ„", "ê³¼ì¥", "ì¡°ì‘", "ìœ„í—˜", "ê±°ì§“"]):
            st.error("âš ï¸ ê²½ê³ : í—ˆìœ„ ë˜ëŠ” ê³¼ì¥ ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.success("âœ… ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²Œì‹œë¬¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.")
